{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for data cleaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the input CSV file name\n",
    "input_csv_file = 'Global_Superstore2.xlsx - Sheet1.csv'\n",
    "# Define the output CSV file name for the cleaned data\n",
    "output_csv_file = 'Global_Superstore_Cleaned.csv'\n",
    "\n",
    "# --- 1. Load the Dataset ---\n",
    "print(f\"Loading data from '{input_csv_file}'...\")\n",
    "try:\n",
    "    df = pd.read_csv(input_csv_file)\n",
    "    print(\"Data loaded successfully. Initial shape:\", df.shape)\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nColumn information and data types:\")\n",
    "    df.info()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{input_csv_file}' was not found. Please ensure it's in the same directory.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Make a copy to preserve the original DataFrame if needed for comparison\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# --- 2. Handle Missing Values ---\n",
    "print(\"\\n--- Handling Missing Values ---\")\n",
    "print(\"Missing values before handling:\")\n",
    "print(df_cleaned.isnull().sum()[df_cleaned.isnull().sum() > 0])\n",
    "\n",
    "# Separate columns by data type for specific handling\n",
    "numerical_cols = df_cleaned.select_dtypes(include=np.number).columns\n",
    "categorical_cols = df_cleaned.select_dtypes(include='object').columns\n",
    "\n",
    "# Strategy 1: Impute numerical columns with the mean\n",
    "# You could also use .median() for skewed data\n",
    "for col in numerical_cols:\n",
    "    if df_cleaned[col].isnull().any():\n",
    "        mean_val = df_cleaned[col].mean()\n",
    "        df_cleaned[col].fillna(mean_val, inplace=True)\n",
    "        print(f\"Filled missing values in numerical column '{col}' with mean: {mean_val:.2f}\")\n",
    "\n",
    "# Strategy 2: Impute categorical columns with a placeholder 'Unknown'\n",
    "# Alternatively, you could use df_cleaned[col].mode()[0] to fill with the most frequent value\n",
    "for col in categorical_cols:\n",
    "    if df_cleaned[col].isnull().any():\n",
    "        df_cleaned[col].fillna('Unknown', inplace=True)\n",
    "        print(f\"Filled missing values in categorical column '{col}' with 'Unknown'\")\n",
    "\n",
    "print(\"\\nMissing values after handling:\")\n",
    "print(df_cleaned.isnull().sum()[df_cleaned.isnull().sum() > 0])\n",
    "if df_cleaned.isnull().sum().sum() == 0:\n",
    "    print(\"No missing values remaining.\")\n",
    "\n",
    "# --- 3. Remove Duplicates ---\n",
    "print(\"\\n--- Removing Duplicates ---\")\n",
    "initial_rows = df_cleaned.shape[0]\n",
    "df_cleaned.drop_duplicates(inplace=True)\n",
    "rows_after_duplicates = df_cleaned.shape[0]\n",
    "duplicates_removed = initial_rows - rows_after_duplicates\n",
    "print(f\"Removed {duplicates_removed} duplicate rows.\")\n",
    "print(f\"Dataset shape after removing duplicates: {df_cleaned.shape}\")\n",
    "\n",
    "# --- 4. Detect and Handle Outliers (using IQR method) ---\n",
    "print(\"\\n--- Detecting and Handling Outliers (IQR Method) ---\")\n",
    "\n",
    "# We will apply outlier handling only to numerical columns\n",
    "outlier_columns = []\n",
    "for col in numerical_cols:\n",
    "    # Skip columns that might be IDs or have very few unique values if they don't represent quantities\n",
    "    # For example, if 'Row ID' was numerical, we wouldn't treat it for outliers.\n",
    "    # This example assumes all numerical columns are candidates for outlier detection.\n",
    "    if df_cleaned[col].nunique() > 2: # Only consider columns with more than 2 unique values\n",
    "        Q1 = df_cleaned[col].quantile(0.25)\n",
    "        Q3 = df_cleaned[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Identify outliers\n",
    "        outliers = df_cleaned[(df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)]\n",
    "\n",
    "        if not outliers.empty:\n",
    "            outlier_columns.append(col)\n",
    "            print(f\"Column '{col}': Found {len(outliers)} outliers.\")\n",
    "            print(f\"  Lower Bound: {lower_bound:.2f}, Upper Bound: {upper_bound:.2f}\")\n",
    "\n",
    "            # Strategy for handling outliers: Capping/Flooring\n",
    "            # Values below lower_bound are set to lower_bound\n",
    "            # Values above upper_bound are set to upper_bound\n",
    "            df_cleaned[col] = np.where(df_cleaned[col] < lower_bound, lower_bound, df_cleaned[col])\n",
    "            df_cleaned[col] = np.where(df_cleaned[col] > upper_bound, upper_bound, df_cleaned[col])\n",
    "            print(f\"  Outliers in '{col}' have been capped/floored.\")\n",
    "        else:\n",
    "            print(f\"Column '{col}': No significant outliers detected.\")\n",
    "\n",
    "if not outlier_columns:\n",
    "    print(\"No columns with significant outliers were found or handled.\")\n",
    "else:\n",
    "    print(f\"\\nOutlier handling applied to columns: {', '.join(outlier_columns)}\")\n",
    "\n",
    "\n",
    "# --- 5. Save the Cleaned Dataset ---\n",
    "print(f\"\\n--- Saving Cleaned Data ---\")\n",
    "try:\n",
    "    df_cleaned.to_csv(output_csv_file, index=False)\n",
    "    print(f\"Cleaned data saved successfully to '{output_csv_file}'.\")\n",
    "    print(f\"Final shape of the cleaned dataset: {df_cleaned.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the cleaned CSV: {e}\")\n",
    "\n",
    "print(\"\\n--- Data Cleaning Process Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff400cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for stasticial analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the input CSV file name (this should be the output from the cleaning script)\n",
    "input_cleaned_csv_file = 'Global_Superstore_Cleaned.csv'\n",
    "\n",
    "# --- 1. Load the Cleaned Dataset ---\n",
    "print(f\"Loading cleaned data from '{input_cleaned_csv_file}' for statistical analysis...\")\n",
    "try:\n",
    "    df_analyzed = pd.read_csv(input_cleaned_csv_file)\n",
    "    print(\"Cleaned data loaded successfully. Shape:\", df_analyzed.shape)\n",
    "    print(\"\\nFirst 5 rows of the cleaned dataset:\")\n",
    "    print(df_analyzed.head())\n",
    "    print(\"\\nColumn information and data types:\")\n",
    "    df_analyzed.info()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The cleaned file '{input_cleaned_csv_file}' was not found. Please run the data cleaning script first.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the cleaned CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Identify numerical columns for statistical analysis\n",
    "numerical_cols_analysis = df_analyzed.select_dtypes(include=np.number).columns\n",
    "\n",
    "# --- 2. Statistical Analysis ---\n",
    "print(\"\\n--- Statistical Analysis ---\")\n",
    "\n",
    "if not numerical_cols_analysis.empty:\n",
    "    print(\"\\nDescriptive Statistics for Numerical Columns:\")\n",
    "    # Compute mean, median, standard deviation, and variance\n",
    "    descriptive_stats = df_analyzed[numerical_cols_analysis].agg(['mean', 'median', 'std', 'var'])\n",
    "    print(descriptive_stats)\n",
    "\n",
    "    print(\"\\nCorrelation Matrix for Numerical Columns:\")\n",
    "    # Compute correlations between numerical variables\n",
    "    correlation_matrix = df_analyzed[numerical_cols_analysis].corr()\n",
    "    print(correlation_matrix)\n",
    "else:\n",
    "    print(\"No numerical columns found for statistical analysis in the cleaned dataset.\")\n",
    "\n",
    "print(\"\\n--- Statistical Analysis Process Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9acb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Visualization:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the input CSV file name (this should be the output from the cleaning script)\n",
    "input_cleaned_csv_file = 'Global_Superstore_Cleaned.csv'\n",
    "\n",
    "# --- 1. Load the Cleaned Dataset ---\n",
    "print(f\"Loading cleaned data from '{input_cleaned_csv_file}' for data visualization...\")\n",
    "try:\n",
    "    df_visualized = pd.read_csv(input_cleaned_csv_file)\n",
    "    print(\"Cleaned data loaded successfully. Shape:\", df_visualized.shape)\n",
    "    print(\"\\nFirst 5 rows of the cleaned dataset:\")\n",
    "    print(df_visualized.head())\n",
    "    print(\"\\nColumn information and data types:\")\n",
    "    df_visualized.info()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The cleaned file '{input_cleaned_csv_file}' was not found. Please run the data cleaning script first.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the cleaned CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Identify numerical columns for visualization\n",
    "numerical_cols_visualization = df_visualized.select_dtypes(include=np.number).columns\n",
    "\n",
    "# --- 2. Data Visualization ---\n",
    "print(\"\\n--- Generating Data Visualizations ---\")\n",
    "\n",
    "if not numerical_cols_visualization.empty:\n",
    "    # --- 2.1. Histograms for Numerical Data Distribution ---\n",
    "    print(\"\\nGenerating Histograms for Numerical Data Distribution...\")\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, col in enumerate(numerical_cols_visualization):\n",
    "        plt.subplot(len(numerical_cols_visualization) // 3 + 1, 3, i + 1)\n",
    "        sns.histplot(df_visualized[col], kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Histograms displayed.\")\n",
    "\n",
    "    # --- 2.2. Boxplots to Identify Outliers in Continuous Variables ---\n",
    "    print(\"\\nGenerating Boxplots to Identify Outliers...\")\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, col in enumerate(numerical_cols_visualization):\n",
    "        plt.subplot(len(numerical_cols_visualization) // 3 + 1, 3, i + 1)\n",
    "        sns.boxplot(y=df_visualized[col])\n",
    "        plt.title(f'Boxplot of {col}')\n",
    "        plt.ylabel(col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Boxplots displayed.\")\n",
    "\n",
    "    # --- 2.3. Heatmap to Visualize Correlations ---\n",
    "    print(\"\\nGenerating Heatmap for Correlations...\")\n",
    "    correlation_matrix = df_visualized[numerical_cols_visualization].corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.show()\n",
    "    print(\"Correlation Heatmap displayed.\")\n",
    "\n",
    "else:\n",
    "    print(\"No numerical columns found for data visualization.\")\n",
    "\n",
    "print(\"\\n--- Data Visualization Process Complete ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
