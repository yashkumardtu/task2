{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565fe2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'sales_data_sample.csv'\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='latin1') # Using latin1 encoding as it's common for such datasets\n",
    "    print(f\"Dataset '{file_path}' loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found. Please ensure it's in the correct directory.\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {e}\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\n--- Dataset Information ---\")\n",
    "\n",
    "    # Display the shape of the dataset\n",
    "    print(\"\\nShape of the dataset (rows, columns):\")\n",
    "    print(df.shape)\n",
    "\n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing values in each column:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Display data types\n",
    "    print(\"\\nData types of each column:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    # Display the first 5 rows to get a glimpse of the data\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Display basic descriptive statistics\n",
    "    print(\"\\nBasic descriptive statistics:\")\n",
    "    print(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'sales_data_sample.csv'\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='latin1')\n",
    "    print(f\"Dataset '{file_path}' loaded successfully for cleaning.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found. Please ensure it's in the correct directory.\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {e}\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\n--- Original Dataset Information (before cleaning) ---\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    print(\"Original missing values:\\n\", df.isnull().sum().sum()) # Total missing values\n",
    "    print(\"Original duplicate rows:\", df.duplicated().sum())\n",
    "\n",
    "    # 1. Remove duplicates\n",
    "    initial_rows = df.shape[0]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    rows_after_duplicates = df.shape[0]\n",
    "    print(f\"\\n--- Duplicates Removal ---\")\n",
    "    print(f\"Removed {initial_rows - rows_after_duplicates} duplicate rows.\")\n",
    "    print(f\"New shape after removing duplicates: {df.shape}\")\n",
    "\n",
    "    # 2. Fill missing values\n",
    "    print(\"\\n--- Handling Missing Values ---\")\n",
    "    # Identify numerical and categorical columns for imputation\n",
    "    numerical_cols = df.select_dtypes(include=np.number).columns\n",
    "    categorical_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "    print(\"\\nMissing values before imputation:\")\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0]) # Show only columns with missing values\n",
    "\n",
    "    # Fill numerical missing values with the median\n",
    "    for col in numerical_cols:\n",
    "        if df[col].isnull().any():\n",
    "            median_val = df[col].median()\n",
    "            df[col].fillna(median_val, inplace=True)\n",
    "            print(f\"Filled missing values in numerical column '{col}' with median: {median_val}\")\n",
    "\n",
    "    # Fill categorical missing values with the mode\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().any():\n",
    "            # Check if mode is unique; if not, pick the first one\n",
    "            mode_val = df[col].mode()[0]\n",
    "            df[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"Filled missing values in categorical column '{col}' with mode: {mode_val}\")\n",
    "\n",
    "    print(\"\\nMissing values after imputation:\")\n",
    "    print(df.isnull().sum()[df.isnull().sum() > 0]) # Should show no columns with missing values if successful\n",
    "    if df.isnull().sum().sum() == 0:\n",
    "        print(\"All missing values have been handled.\")\n",
    "    else:\n",
    "        print(\"Warning: Some missing values still exist after imputation.\")\n",
    "\n",
    "\n",
    "    # 3. Convert 'ORDERDATE' column to datetime object\n",
    "    # Assuming 'ORDERDATE' is the correct column name for date information\n",
    "    date_column = 'ORDERDATE'\n",
    "    if date_column in df.columns:\n",
    "        print(f\"\\n--- Converting '{date_column}' to Datetime ---\")\n",
    "        original_dtype = df[date_column].dtype\n",
    "        try:\n",
    "            # Attempt to convert with common formats, coerce errors to NaT\n",
    "            df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "            print(f\"'{date_column}' successfully converted from {original_dtype} to {df[date_column].dtype}.\")\n",
    "\n",
    "            # Check for any NaT values introduced during conversion (if original data was messy)\n",
    "            if df[date_column].isnull().any():\n",
    "                nan_dates = df[date_column].isnull().sum()\n",
    "                print(f\"Warning: {nan_dates} values in '{date_column}' could not be converted and were set to NaT.\")\n",
    "                # Optionally, fill these NaT values if needed, e.g., with the mode date or a specific date\n",
    "                # df[date_column].fillna(df[date_column].mode()[0], inplace=True)\n",
    "                # print(f\"Filled {nan_dates} NaT values in '{date_column}' with its mode.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting '{date_column}' to datetime: {e}\")\n",
    "    else:\n",
    "        print(f\"\\nWarning: Column '{date_column}' not found in the dataset. Skipping date conversion.\")\n",
    "\n",
    "    print(\"\\n--- Dataset Information After Cleaning ---\")\n",
    "    print(f\"Final shape: {df.shape}\")\n",
    "    print(\"Final missing values:\\n\", df.isnull().sum().sum())\n",
    "    print(\"Final data types (snippet):\\n\", df.dtypes.head())\n",
    "    print(\"\\nFirst 5 rows of the cleaned dataset:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18347903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'sales_data_sample.csv'\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='latin1')\n",
    "    print(f\"Dataset '{file_path}' loaded successfully for EDA.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found. Please ensure it's in the correct directory.\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {e}\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    # --- Data Cleaning (Re-applying steps from data_cleaning immersive for consistency) ---\n",
    "    print(\"\\n--- Applying Data Cleaning Steps ---\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    initial_rows = df.shape[0]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"Removed {initial_rows - df.shape[0]} duplicate rows.\")\n",
    "\n",
    "    # Fill missing values\n",
    "    numerical_cols = df.select_dtypes(include=np.number).columns\n",
    "    categorical_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "            # print(f\"Filled missing values in numerical column '{col}' with median.\")\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            # print(f\"Filled missing values in categorical column '{col}' with mode.\")\n",
    "    print(\"Missing values handled.\")\n",
    "\n",
    "    # Convert 'ORDERDATE' column to datetime object\n",
    "    date_column = 'ORDERDATE'\n",
    "    if date_column in df.columns:\n",
    "        df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "        # Handle NaT values if any were introduced during conversion\n",
    "        if df[date_column].isnull().any():\n",
    "            # For simplicity in EDA, we'll drop rows with NaT dates or fill with a sensible default\n",
    "            # Here, we'll drop them to ensure valid dates for time series analysis\n",
    "            df.dropna(subset=[date_column], inplace=True)\n",
    "            print(f\"Dropped rows with unconvertible '{date_column}' values.\")\n",
    "        print(f\"'{date_column}' converted to datetime.\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{date_column}' not found. Time series analysis might be affected.\")\n",
    "\n",
    "    print(\"\\n--- Starting Exploratory Data Analysis ---\")\n",
    "\n",
    "    # 1. Plot time series graphs to observe trends in Sales over time.\n",
    "    if date_column in df.columns and 'SALES' in df.columns:\n",
    "        print(f\"\\nGenerating Time Series Plot for Sales over {date_column}...\")\n",
    "        # Aggregate sales by date\n",
    "        sales_over_time = df.groupby(date_column)['SALES'].sum().reset_index()\n",
    "        sales_over_time = sales_over_time.sort_values(by=date_column)\n",
    "\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        sns.lineplot(x=date_column, y='SALES', data=sales_over_time)\n",
    "        plt.title('Total Sales Over Time')\n",
    "        plt.xlabel('Order Date')\n",
    "        plt.ylabel('Total Sales')\n",
    "        plt.grid(True)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping Time Series Plot: 'ORDERDATE' or 'SALES' column not found.\")\n",
    "\n",
    "    # 2. Use scatter plots to study the relationship between Profit and Discount.\n",
    "    if 'PROFIT' in df.columns and 'DISCOUNT' in df.columns:\n",
    "        print(\"\\nGenerating Scatter Plot for Profit vs. Discount...\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x='DISCOUNT', y='PROFIT', data=df, alpha=0.6)\n",
    "        plt.title('Profit vs. Discount')\n",
    "        plt.xlabel('Discount')\n",
    "        plt.ylabel('Profit')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping Scatter Plot: 'PROFIT' or 'DISCOUNT' column not found.\")\n",
    "\n",
    "\n",
    "    # 3. Visualize sales distribution by Region and Category using bar plots.\n",
    "    # Assuming 'REGION' and 'PRODUCTLINE' are relevant categorical columns for sales distribution\n",
    "    # You might need to adjust 'PRODUCTLINE' to 'CATEGORY' if your dataset has a 'CATEGORY' column instead.\n",
    "    # Check your df.columns to confirm.\n",
    "\n",
    "    if 'REGION' in df.columns and 'SALES' in df.columns:\n",
    "        print(\"\\nGenerating Bar Plot for Sales by Region...\")\n",
    "        sales_by_region = df.groupby('REGION')['SALES'].sum().sort_values(ascending=False).reset_index()\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='REGION', y='SALES', data=sales_by_region, palette='viridis')\n",
    "        plt.title('Total Sales by Region')\n",
    "        plt.xlabel('Region')\n",
    "        plt.ylabel('Total Sales')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping Sales by Region Bar Plot: 'REGION' or 'SALES' column not found.\")\n",
    "\n",
    "    if 'PRODUCTLINE' in df.columns and 'SALES' in df.columns:\n",
    "        print(\"\\nGenerating Bar Plot for Sales by Product Line...\")\n",
    "        sales_by_productline = df.groupby('PRODUCTLINE')['SALES'].sum().sort_values(ascending=False).reset_index()\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        sns.barplot(x='PRODUCTLINE', y='SALES', data=sales_by_productline, palette='magma')\n",
    "        plt.title('Total Sales by Product Line')\n",
    "        plt.xlabel('Product Line')\n",
    "        plt.ylabel('Total Sales')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping Sales by Product Line Bar Plot: 'PRODUCTLINE' or 'SALES' column not found.\")\n",
    "\n",
    "    print(\"\\nEDA plots generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4721bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictive Modeling:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Define the file path\n",
    "file_path = 'sales_data_sample.csv'\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='latin1')\n",
    "    print(f\"Dataset '{file_path}' loaded successfully for Predictive Modeling.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found. Please ensure it's in the correct directory.\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the dataset: {e}\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    # --- Data Cleaning (Re-applying steps for consistency and self-contained script) ---\n",
    "    print(\"\\n--- Applying Data Cleaning Steps ---\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    initial_rows = df.shape[0]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"Removed {initial_rows - df.shape[0]} duplicate rows.\")\n",
    "\n",
    "    # Fill missing values\n",
    "    numerical_cols = df.select_dtypes(include=np.number).columns\n",
    "    categorical_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "            # print(f\"Filled missing values in numerical column '{col}' with median.\")\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().any():\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "            # print(f\"Filled missing values in categorical column '{col}' with mode.\")\n",
    "    print(\"Missing values handled.\")\n",
    "\n",
    "    # Convert 'ORDERDATE' column to datetime object (though not directly used in this specific model, good practice)\n",
    "    date_column = 'ORDERDATE'\n",
    "    if date_column in df.columns:\n",
    "        df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "        if df[date_column].isnull().any():\n",
    "            df.dropna(subset=[date_column], inplace=True)\n",
    "            print(f\"Dropped rows with unconvertible '{date_column}' values.\")\n",
    "        print(f\"'{date_column}' converted to datetime.\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{date_column}' not found.\")\n",
    "\n",
    "    print(\"\\n--- Starting Predictive Modeling (Linear Regression) ---\")\n",
    "\n",
    "    # Define features (X) and target (y)\n",
    "    features = ['PROFIT', 'DISCOUNT']\n",
    "    target = 'SALES'\n",
    "\n",
    "    # Check if required columns exist\n",
    "    if all(col in df.columns for col in features) and target in df.columns:\n",
    "        X = df[features]\n",
    "        y = df[target]\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        # Using a test size of 20% and a random state for reproducibility\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        print(f\"Data split into training ({len(X_train)} samples) and testing ({len(X_test)} samples) sets.\")\n",
    "\n",
    "        # Initialize and train the Linear Regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        print(\"Linear Regression model trained successfully.\")\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse) # Root Mean Squared Error\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        print(\"\\n--- Model Evaluation Metrics ---\")\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "        print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "        print(f\"R-squared (R2): {r2:.2f}\")\n",
    "\n",
    "        # Display model coefficients\n",
    "        print(\"\\n--- Model Coefficients ---\")\n",
    "        for i, feature in enumerate(features):\n",
    "            print(f\"{feature}: {model.coef_[i]:.2f}\")\n",
    "        print(f\"Intercept: {model.intercept_:.2f}\")\n",
    "\n",
    "        # Optional: Plotting actual vs. predicted values\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "        # Add a line representing perfect prediction\n",
    "        plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "        plt.title('Actual vs. Predicted Sales')\n",
    "        plt.xlabel('Actual Sales')\n",
    "        plt.ylabel('Predicted Sales')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping Predictive Modeling: Required columns ({features} and {target}) not found in the dataset.\")\n",
    "\n",
    "    print(\"\\nPredictive modeling complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
